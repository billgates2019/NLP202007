{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "516a50fb-7575-4a9b-8ac3-0a95fa224055"
   },
   "source": [
    "# 使用gensim训练word2vec\n",
    "\n",
    "本DEMO只使用部分数据，使用全部数据预训练的词向量地址：  \n",
    "\n",
    "链接: https://pan.baidu.com/s/1ewlck3zwXVQuAzraZ26Euw 提取码: qbpr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "uuid": "8c02a953-9d2e-468b-8128-e7d82beafa68"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f1eb62ef270>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)-15s %(levelname)s: %(message)s')\n",
    "\n",
    "# set seed \n",
    "seed = 666\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "uuid": "e324c8e3-e1ab-441b-942d-e0b96212721f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-31 15:15:28,243 INFO: Fold lens [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]\n"
     ]
    }
   ],
   "source": [
    "# split data to 10 fold\n",
    "fold_num = 10\n",
    "data_file = '../data/train_set.csv'\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def all_data2fold(fold_num, num=10000):\n",
    "    fold_data = []\n",
    "    f = pd.read_csv(data_file, sep='\\t', encoding='UTF-8')\n",
    "    texts = f['text'].tolist()[:num]\n",
    "    labels = f['label'].tolist()[:num]\n",
    "\n",
    "    total = len(labels)\n",
    "\n",
    "    index = list(range(total))\n",
    "    np.random.shuffle(index)\n",
    "\n",
    "    all_texts = []\n",
    "    all_labels = []\n",
    "    for i in index:\n",
    "        all_texts.append(texts[i])\n",
    "        all_labels.append(labels[i])\n",
    "\n",
    "    label2id = {}\n",
    "    for i in range(total):\n",
    "        label = str(all_labels[i])\n",
    "        if label not in label2id:\n",
    "            label2id[label] = [i]\n",
    "        else:\n",
    "            label2id[label].append(i)\n",
    "\n",
    "    all_index = [[] for _ in range(fold_num)]\n",
    "    for label, data in label2id.items():\n",
    "        # print(label, len(data))\n",
    "        batch_size = int(len(data) / fold_num)\n",
    "        other = len(data) - batch_size * fold_num\n",
    "        for i in range(fold_num):\n",
    "            cur_batch_size = batch_size + 1 if i < other else batch_size\n",
    "            # print(cur_batch_size)\n",
    "            batch_data = [data[i * batch_size + b] for b in range(cur_batch_size)]\n",
    "            all_index[i].extend(batch_data)\n",
    "\n",
    "    batch_size = int(total / fold_num)\n",
    "    other_texts = []\n",
    "    other_labels = []\n",
    "    other_num = 0\n",
    "    start = 0\n",
    "    for fold in range(fold_num):\n",
    "        num = len(all_index[fold])\n",
    "        texts = [all_texts[i] for i in all_index[fold]]\n",
    "        labels = [all_labels[i] for i in all_index[fold]]\n",
    "\n",
    "        if num > batch_size:\n",
    "            fold_texts = texts[:batch_size]\n",
    "            other_texts.extend(texts[batch_size:])\n",
    "            fold_labels = labels[:batch_size]\n",
    "            other_labels.extend(labels[batch_size:])\n",
    "            other_num += num - batch_size\n",
    "        elif num < batch_size:\n",
    "            end = start + batch_size - num\n",
    "            fold_texts = texts + other_texts[start: end]\n",
    "            fold_labels = labels + other_labels[start: end]\n",
    "            start = end\n",
    "        else:\n",
    "            fold_texts = texts\n",
    "            fold_labels = labels\n",
    "\n",
    "        assert batch_size == len(fold_labels)\n",
    "\n",
    "        # shuffle\n",
    "        index = list(range(batch_size))\n",
    "        np.random.shuffle(index)\n",
    "\n",
    "        shuffle_fold_texts = []\n",
    "        shuffle_fold_labels = []\n",
    "        for i in index:\n",
    "            shuffle_fold_texts.append(fold_texts[i])\n",
    "            shuffle_fold_labels.append(fold_labels[i])\n",
    "\n",
    "        data = {'label': shuffle_fold_labels, 'text': shuffle_fold_texts}\n",
    "        fold_data.append(data)\n",
    "\n",
    "    logging.info(\"Fold lens %s\", str([len(data['label']) for data in fold_data]))\n",
    "\n",
    "    return fold_data\n",
    "\n",
    "\n",
    "fold_data = all_data2fold(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "uuid": "3f32eae5-61ba-4841-a41a-f65ba8d63bc3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-31 15:15:39,440 INFO: Total 9000 docs.\n"
     ]
    }
   ],
   "source": [
    "# build train data for word2vec\n",
    "fold_id = 9\n",
    "\n",
    "train_texts = []\n",
    "for i in range(0, fold_id):\n",
    "    data = fold_data[i]\n",
    "    train_texts.extend(data['text'])\n",
    "    \n",
    "logging.info('Total %d docs.' % len(train_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "uuid": "70ef6d13-3a49-4c89-9fd3-3d2a172932f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Looking in indexes: http://yum.tbsite.net/pypi/simple/\n",
      "Collecting gensim\n",
      "  Downloading http://yum.tbsite.net/pypi/packages/2b/e0/fa6326251692056dc880a64eb22117e03269906ba55a6864864d24ec8b4e/gensim-3.8.3-cp36-cp36m-manylinux1_x86_64.whl (24.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.2 MB 146 kB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.6/site-packages (from gensim) (1.16.0)\n",
      "Requirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.6/site-packages (from gensim) (1.11.0)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading http://yum.tbsite.net/pypi/packages/0b/8e/464b06f5efd26f2dc16ce7bd1662c2f31cadf9104fdbcbf5994674cc3a51/smart_open-2.1.0.tar.gz (116 kB)\n",
      "\u001b[K     |████████████████████████████████| 116 kB 16.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.6/site-packages (from gensim) (1.3.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (2.18.4)\n",
      "Collecting boto\n",
      "  Downloading http://yum.tbsite.net/pypi/packages/23/10/c0b78c27298029e4454a472a1919bde20cb182dab1662cec7f2ca1dcc523/boto-2.49.0-py2.py3-none-any.whl (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 1.1 MB/s \n",
      "\u001b[?25hCollecting boto3\n",
      "  Downloading http://yum.tbsite.net/pypi/packages/bd/83/22bc643490012047408bfeec8422c79ba54ecc089e70c946cf1686e15084/boto3-1.14.31-py2.py3-none-any.whl (129 kB)\n",
      "\u001b[K     |████████████████████████████████| 129 kB 18.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (2017.11.5)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.9.4)\n",
      "Collecting botocore<1.18.0,>=1.17.31\n",
      "  Downloading http://yum.tbsite.net/pypi/packages/ef/09/ad453cb97d14ba9434a863dbd12243e891fb13e22259fa9d30a904093fab/botocore-1.17.31-py2.py3-none-any.whl (6.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.4 MB 15.1 MB/s \n",
      "\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading http://yum.tbsite.net/pypi/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)\n",
      "\u001b[K     |████████████████████████████████| 69 kB 14.8 MB/s \n",
      "\u001b[?25hCollecting docutils<0.16,>=0.10\n",
      "  Downloading http://yum.tbsite.net/pypi/packages/22/cd/a6aa959dca619918ccb55023b4cb151949c64d4d5d55b3f4ffd7eee0c6e8/docutils-0.15.2-py3-none-any.whl (547 kB)\n",
      "\u001b[K     |████████████████████████████████| 547 kB 345 kB/s \n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.31->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for smart-open: filename=smart_open-2.1.0-py3-none-any.whl size=117765 sha256=0f2da4436a9b991a2d3175366b30401eb4f1ae491cd8654b4349825ec4664bf4\n",
      "  Stored in directory: /home/admin/.cache/pip/wheels/11/f1/f8/bd5c0099e76ca1dc7bbe3c451ca8eaa6d85bf39feaefc33a66\n",
      "Successfully built smart-open\n",
      "Installing collected packages: boto, docutils, botocore, s3transfer, boto3, smart-open, gensim\n",
      "Successfully installed boto-2.49.0 boto3-1.14.31 botocore-1.17.31 docutils-0.15.2 gensim-3.8.3 s3transfer-0.3.3 smart-open-2.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "uuid": "55321072-aa13-4acd-b2cf-afa420a2c08b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-31 15:19:37,159 INFO: Start training...\n",
      "2020-07-31 15:19:42,698 INFO: collecting all words and their counts\n",
      "2020-07-31 15:19:42,700 INFO: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-07-31 15:19:43,969 INFO: collected 5295 word types from a corpus of 8191447 raw words and 9000 sentences\n",
      "2020-07-31 15:19:43,970 INFO: Loading a fresh vocabulary\n",
      "2020-07-31 15:19:44,078 INFO: effective_min_count=5 retains 4335 unique words (81% of original 5295, drops 960)\n",
      "2020-07-31 15:19:44,079 INFO: effective_min_count=5 leaves 8189498 word corpus (99% of original 8191447, drops 1949)\n",
      "2020-07-31 15:19:44,094 INFO: deleting the raw counts dictionary of 5295 items\n",
      "2020-07-31 15:19:44,095 INFO: sample=0.001 downsamples 61 most-common words\n",
      "2020-07-31 15:19:44,096 INFO: downsampling leaves estimated 7070438 word corpus (86.3% of prior 8189498)\n",
      "2020-07-31 15:19:44,107 INFO: estimated required memory for 4335 words and 100 dimensions: 5635500 bytes\n",
      "2020-07-31 15:19:44,107 INFO: resetting layer weights\n",
      "2020-07-31 15:19:44,162 INFO: training model with 8 workers on 4335 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-07-31 15:19:45,170 INFO: EPOCH 1 - PROGRESS: at 37.30% examples, 2623102 words/s, in_qsize 14, out_qsize 1\n",
      "2020-07-31 15:19:46,171 INFO: EPOCH 1 - PROGRESS: at 73.83% examples, 2596548 words/s, in_qsize 15, out_qsize 0\n",
      "2020-07-31 15:19:46,829 INFO: worker thread finished; awaiting finish of 7 more threads\n",
      "2020-07-31 15:19:46,831 INFO: worker thread finished; awaiting finish of 6 more threads\n",
      "2020-07-31 15:19:46,833 INFO: worker thread finished; awaiting finish of 5 more threads\n",
      "2020-07-31 15:19:46,833 INFO: worker thread finished; awaiting finish of 4 more threads\n",
      "2020-07-31 15:19:46,836 INFO: worker thread finished; awaiting finish of 3 more threads\n",
      "2020-07-31 15:19:46,837 INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-31 15:19:46,842 INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-31 15:19:46,844 INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-31 15:19:46,844 INFO: EPOCH - 1 : training on 8191447 raw words (7022091 effective words) took 2.7s, 2622292 effective words/s\n",
      "2020-07-31 15:19:47,855 INFO: EPOCH 2 - PROGRESS: at 34.18% examples, 2384309 words/s, in_qsize 15, out_qsize 0\n",
      "2020-07-31 15:19:48,857 INFO: EPOCH 2 - PROGRESS: at 67.89% examples, 2384947 words/s, in_qsize 15, out_qsize 0\n",
      "2020-07-31 15:19:49,762 INFO: worker thread finished; awaiting finish of 7 more threads\n",
      "2020-07-31 15:19:49,764 INFO: worker thread finished; awaiting finish of 6 more threads\n",
      "2020-07-31 15:19:49,764 INFO: worker thread finished; awaiting finish of 5 more threads\n",
      "2020-07-31 15:19:49,765 INFO: worker thread finished; awaiting finish of 4 more threads\n",
      "2020-07-31 15:19:49,771 INFO: worker thread finished; awaiting finish of 3 more threads\n",
      "2020-07-31 15:19:49,772 INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-31 15:19:49,779 INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-31 15:19:49,781 INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-31 15:19:49,782 INFO: EPOCH - 2 : training on 8191447 raw words (7022492 effective words) took 2.9s, 2393888 effective words/s\n",
      "2020-07-31 15:19:50,788 INFO: EPOCH 3 - PROGRESS: at 33.80% examples, 2367388 words/s, in_qsize 15, out_qsize 0\n",
      "2020-07-31 15:19:51,790 INFO: EPOCH 3 - PROGRESS: at 67.62% examples, 2374259 words/s, in_qsize 15, out_qsize 0\n",
      "2020-07-31 15:19:52,714 INFO: worker thread finished; awaiting finish of 7 more threads\n",
      "2020-07-31 15:19:52,715 INFO: worker thread finished; awaiting finish of 6 more threads\n",
      "2020-07-31 15:19:52,717 INFO: worker thread finished; awaiting finish of 5 more threads\n",
      "2020-07-31 15:19:52,718 INFO: worker thread finished; awaiting finish of 4 more threads\n",
      "2020-07-31 15:19:52,724 INFO: worker thread finished; awaiting finish of 3 more threads\n",
      "2020-07-31 15:19:52,725 INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-31 15:19:52,729 INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-31 15:19:52,733 INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-31 15:19:52,733 INFO: EPOCH - 3 : training on 8191447 raw words (7021017 effective words) took 2.9s, 2382362 effective words/s\n",
      "2020-07-31 15:19:53,742 INFO: EPOCH 4 - PROGRESS: at 33.54% examples, 2346050 words/s, in_qsize 15, out_qsize 0\n",
      "2020-07-31 15:19:54,744 INFO: EPOCH 4 - PROGRESS: at 68.42% examples, 2404592 words/s, in_qsize 15, out_qsize 0\n",
      "2020-07-31 15:19:55,617 INFO: worker thread finished; awaiting finish of 7 more threads\n",
      "2020-07-31 15:19:55,618 INFO: worker thread finished; awaiting finish of 6 more threads\n",
      "2020-07-31 15:19:55,620 INFO: worker thread finished; awaiting finish of 5 more threads\n",
      "2020-07-31 15:19:55,624 INFO: worker thread finished; awaiting finish of 4 more threads\n",
      "2020-07-31 15:19:55,625 INFO: worker thread finished; awaiting finish of 3 more threads\n",
      "2020-07-31 15:19:55,629 INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-31 15:19:55,629 INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-31 15:19:55,632 INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-31 15:19:55,633 INFO: EPOCH - 4 : training on 8191447 raw words (7022196 effective words) took 2.9s, 2425887 effective words/s\n",
      "2020-07-31 15:19:56,639 INFO: EPOCH 5 - PROGRESS: at 32.98% examples, 2311431 words/s, in_qsize 15, out_qsize 0\n",
      "2020-07-31 15:19:57,639 INFO: EPOCH 5 - PROGRESS: at 67.07% examples, 2357800 words/s, in_qsize 15, out_qsize 0\n",
      "2020-07-31 15:19:58,574 INFO: worker thread finished; awaiting finish of 7 more threads\n",
      "2020-07-31 15:19:58,578 INFO: worker thread finished; awaiting finish of 6 more threads\n",
      "2020-07-31 15:19:58,579 INFO: worker thread finished; awaiting finish of 5 more threads\n",
      "2020-07-31 15:19:58,580 INFO: worker thread finished; awaiting finish of 4 more threads\n",
      "2020-07-31 15:19:58,582 INFO: worker thread finished; awaiting finish of 3 more threads\n",
      "2020-07-31 15:19:58,583 INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-31 15:19:58,591 INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-31 15:19:58,595 INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-31 15:19:58,596 INFO: EPOCH - 5 : training on 8191447 raw words (7021976 effective words) took 3.0s, 2372965 effective words/s\n",
      "2020-07-31 15:19:58,596 INFO: training on a 40957235 raw words (35109772 effective words) took 14.4s, 2432408 effective words/s\n",
      "2020-07-31 15:19:58,597 INFO: precomputing L2-norms of word weight vectors\n",
      "2020-07-31 15:19:58,600 INFO: saving Word2Vec object under ./word2vec.bin, separately None\n",
      "2020-07-31 15:19:58,601 INFO: not storing attribute vectors_norm\n",
      "2020-07-31 15:19:58,602 INFO: not storing attribute cum_table\n",
      "2020-07-31 15:19:58,676 INFO: saved ./word2vec.bin\n"
     ]
    }
   ],
   "source": [
    "logging.info('Start training...')\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "num_features = 100     # Word vector dimensionality\n",
    "num_workers = 8       # Number of threads to run in parallel\n",
    "\n",
    "train_texts = list(map(lambda x: list(x.split()), train_texts))\n",
    "model = Word2Vec(train_texts, workers=num_workers, size=num_features)\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# save model\n",
    "model.save(\"./word2vec.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "uuid": "6d2c3f2e-3a28-4348-bf09-a1014f99f063"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-31 15:20:14,112 INFO: loading Word2Vec object from ./word2vec.bin\n",
      "2020-07-31 15:20:14,425 INFO: loading wv recursively from ./word2vec.bin.wv.* with mmap=None\n",
      "2020-07-31 15:20:14,426 INFO: setting ignored attribute vectors_norm to None\n",
      "2020-07-31 15:20:14,427 INFO: loading vocabulary recursively from ./word2vec.bin.vocabulary.* with mmap=None\n",
      "2020-07-31 15:20:14,427 INFO: loading trainables recursively from ./word2vec.bin.trainables.* with mmap=None\n",
      "2020-07-31 15:20:14,428 INFO: setting ignored attribute cum_table to None\n",
      "2020-07-31 15:20:14,429 INFO: loaded ./word2vec.bin\n",
      "2020-07-31 15:20:14,438 INFO: storing 4335x100 projection weights into ./word2vec.txt\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = Word2Vec.load(\"./word2vec.bin\")\n",
    "\n",
    "# convert format\n",
    "model.wv.save_word2vec_format('./word2vec.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "54f5f668-b4e1-4a93-8109-29b441557764"
   },
   "source": [
    "**关于Datawhale：**\n",
    "\n",
    "> Datawhale是一个专注于数据科学与AI领域的开源组织，汇集了众多领域院校和知名企业的优秀学习者，聚合了一群有开源精神和探索精神的团队成员。Datawhale 以“for the learner，和学习者一起成长”为愿景，鼓励真实地展现自我、开放包容、互信互助、敢于试错和勇于担当。同时 Datawhale 用开源的理念去探索开源内容、开源学习和开源方案，赋能人才培养，助力人才成长，建立起人与人，人与知识，人与企业和人与未来的联结。\n",
    "\n",
    "本次新闻文本分类学习，专题知识将在天池分享，详情可关注Datawhale：\n",
    "\n",
    " ![](http://jupter-oss.oss-cn-hangzhou.aliyuncs.com/public/files/image/1095279172547/1584432602983_kAxAvgQpG2.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
